<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Double Backpropagation: A Forgotten Algorithm &middot; Moments of Clarity
    
  </title>

  
  <link rel="canonical" href="https://anish.lakkapragada.com/theoretical/2023/01/01/double-backprop/">
  

  <link rel="stylesheet" href="https://anish.lakkapragada.com/public/css/poole.css">
  <link rel="stylesheet" href="https://anish.lakkapragada.com/public/css/syntax.css">
  <link rel="stylesheet" href="https://anish.lakkapragada.com/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://anish.lakkapragada.com/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="https://anish.lakkapragada.com/public/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="https://anish.lakkapragada.com/atom.xml">

  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'G-MY94EDEHE6', 'auto');
    ga('send', 'pageview');
  </script>
  
</head>


  <body class="theme-base-custom">

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>Writings to describe the thoughts swirling in my head when they slow down and reformulate.</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/">home</a>

    

    
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/about/">about</a>
        
      
    
      
    
      
        
          <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/birds/">birds</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/categories/">categories</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/contact/">leave a note</a>
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/ideas/">ideas</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/notes/">theoretical</a>
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
  </nav>

  <nav class="sidebar-nav">
      <details class="sidebar-nav-item"> 
        <summary> All Posts </summary>  
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/songs/2023/07/23/club-paradise/">Club Paradise by Drake</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/thinking/2023/07/21/hardquestions/">A Hard Question (Partially) Answered, Badly</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/thinking/2023/05/09/tests/">On The Value of Tests</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/thinking/2023/05/04/what-could-go-wrong/">A Growing List of Lucky Things</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/thinking/2023/04/28/reminded/">How did We Get Here?</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/thinking/2023/03/24/why-I-enjoy-school/">Why I Enjoy School</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/birds/2023/03/14/birds-notes/">Bird Notes after a Year</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/random/2023/02/22/naps/">Naps</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/thinking/2023/02/01/update/">A Series of Life Observations</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/thinking/2023/01/11/hedonistic-happy/">Hedonistic Treadmill: What It Means to Be Happy With What You Have</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/theoretical/2023/01/01/double-backprop/">Double Backpropagation: A Forgotten Algorithm</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/thinking/2022/12/22/history/">PSA Reminder: History Actually Happened</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/random/2022/12/10/beards/">My Experience Growing a Beard, And Then Shaving It</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/random/2022/11/24/crawdads-tkam/">Comparison of Where The Crawdads Sing and To Kill a Mockingbird</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/thinking/2022/11/09/word-association/">Food for Thought: Word Predictability</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/theoretical/2022/11/04/latex/">Why Does Latex look so good?</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/random/2022/10/24/act/">Falling Forward, Not Back: Drake</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/theoretical/2022/10/11/gradient-descent-euler/">Gradient Descent Revisited As Euler's Method</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/birds/2022/10/04/birdslist/">Photograph Birds List</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/theoretical/2022/09/26/mltaylorseries-copy/">Hidden Taylor Series in Theoretical Machine Learning</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/thinking/2022/09/25/paretoprinciple/">An explanation of Pareto's Principle</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/coding/2022/09/21/sveltevreact/">Yet Another Comparison of Svelte & React</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/theoretical/2022/09/20/closed-form-logreg/">Attempts at Closed-Form Logistic Regression</a>
          
      </details>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2023. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="home">Moments of Clarity</a>
            <small></small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">Double Backpropagation: A Forgotten Algorithm</h1>
  <span class="post-date">01 Jan 2023</span>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<p><strong>Keywords: Backpropgation, Gradient Descent, Theoretical ML</strong></p>

<p>Hope you are enjoying the new year 2023! It’s been a while since I’ve uploaded a theoretical algorithm so here we are :)</p>

<p>I’ve made a few posts and challenges about typical first-order gradient descent that I think it’s time I move on to other aspects of ML. But before I do, here’s one last gradient-descent-ish article on a variant of standard optimization algorithms and my theory on its potential usefulness in generative models.</p>

<p>Double backpropagation was first created by Le Cun and Drucker in 1992 and since then has been widely dismissed (maybe for computational reasons).Introductory ML courses and books don’t even cover it, so perhaps this article will help explain some things.</p>

<h2 id="double-backpropagation-algorithm">Double Backpropagation Algorithm</h2>

<p>The name for double backpropagation is a little weird because it’s not actually backpropagation (basically gradient descent) applied twice but just an addition of another term in the objective function. For an objective function \(J\) to optimize parameters \(\theta\) on a training set \(X\) , the double backpropagation function \(J'\) is given by:</p>

\[J'(\theta) = J(\theta) + \lambda \lVert \frac{\partial J(\theta)}{\partial X} \rVert^{2}\]

<p>It’s actually pretty simple. The new objective function is just the regular objective functive plus the differentiable L2-norm of the gradient of the regular objective function with respect to the input scaled by some <em>hyperparameter</em> (you choose the value) \(\lambda\).</p>

<h3 id="why">Why?</h3>

<blockquote>
  <p>Double Backpropagation is motivated by forcing the parameters to be as generalizable as possible.</p>
</blockquote>

<p>More concretely, the value of the adjusted function \(J'\) does not care just about the performance of the model with the parameters \(\theta\) but also how stable they are. If the input \(X\) is changed slightly, the model’s performance (objective function \(J\)) should ideally not change much. This is similar to <em>augmentations</em> where we want the predictions (e.g. a cat image predicted as a cat) to be the same regardless of small changes in the input - both double backpropagation + augmentations try to <em>force</em> the model to be generalizable to differing input. This is of course helpful because when a model is deployed, the input the model will encounter may not be similar to the data gathered use for training.</p>

<p>This is why the (partial) derivative in the equation measures the model’s performance (objective function value) sensitivity to small changes in the input \(X\) is added. The algorithm is known as <em>double</em> backpropagation because a <a href="http://luiz.hafemann.ca/libraries/2018/06/22/pytorch-doublebackprop/">gradient of a gradient</a> is utilized.</p>

<h2 id="double-backpropagation-for-smooth-latent-spaces-">Double-Backpropagation for Smooth Latent Spaces (?)</h2>

<p>Just some background here. Generative models (for our purposes) are any type of model which takes in random vector input and generates some novel output. For example, a generative model \(G\) is a function that takes in a random generated vector \(\vec{x}\) and returns an image \(Y\).</p>

<p>Let’s say this generative model is trained through some black-box witchcraft (or a GAN training procedure) to generate images of people through these vectors. In this case, differing vectors will produce different people (e.g. different genders, races, body type, etc.)</p>

<blockquote>
  <p>A latent space is the full extent of values the vector can take, where similar generated images will come from vectors which are near each other in this space.</p>
</blockquote>

<p>A short aside: one of the most underrated parts about machine learning is its ability to frame an infinite space (e.g. vector input values in a generative model) in a way that makes sense: similar output images are generated with vectors that are close to each other. More impressive, these computers don’t really know the meaning of words or images (they just see numbers) but can formulate a good understanding of the world.</p>

<p>Ideally in these models, the mapping of the input vector \(\vec{x}\) to the image \(Y\) learned by \(G\) is <em>smooth</em> - small changes in \(\vec{x}\) shouldn’t lead to drastic changes in \(Y\). Do you see where I am going with this?</p>

</div>


<div class="related">
  <h2>Related posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/songs/2023/07/23/club-paradise/">
            Club Paradise by Drake
            <small>23 Jul 2023</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/thinking/2023/07/21/hardquestions/">
            A Hard Question (Partially) Answered, Badly
            <small>21 Jul 2023</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/thinking/2023/05/09/tests/">
            On The Value of Tests
            <small>09 May 2023</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

<style> 
  .title {
    font-style: italic;
  }
</style> 


      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/public/js/script.js'></script>
  </body>
</html>
