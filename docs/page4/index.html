<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Moments of Clarity &middot; 
    
  </title>

  
  <link rel="canonical" href="https://anish.lakkapragada.com/page4/">
  

  <link rel="stylesheet" href="https://anish.lakkapragada.com/public/css/poole.css">
  <link rel="stylesheet" href="https://anish.lakkapragada.com/public/css/syntax.css">
  <link rel="stylesheet" href="https://anish.lakkapragada.com/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://anish.lakkapragada.com/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="https://anish.lakkapragada.com/public/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="https://anish.lakkapragada.com/atom.xml">

  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'G-MY94EDEHE6', 'auto');
    ga('send', 'pageview');
  </script>
  
</head>


  <body class="theme-base-custom">

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>Writings to describe the thoughts swirling in my head when they slow down and reformulate.</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item active" href="https://anish.lakkapragada.com/">home</a>

    

    
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/about/">about</a>
        
      
    
      
    
      
        
          <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/birds/">birds</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/contact/">leave a note</a>
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/notes/">theoretical</a>
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/tragedies/">tragedies</a>
        
      
    
  </nav>

  <nav class="sidebar-nav">
      <details class="sidebar-nav-item"> 
        <summary> All Posts </summary>  
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/jekyll/update/2023/07/20/hardquestions/">A Hard Question (Partially) Answered, Badly</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/jekyll/update/2023/05/09/tests/">On The Value of Tests</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/jekyll/update/2023/05/04/what-could-go-wrong/">A Growing List of Lucky Things</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/jekyll/update/2023/04/28/reminded/">How did We Get Here?</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/jekyll/update/2023/03/24/why-I-enjoy-school/">Why I Enjoy School</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/jekyll/update/2023/03/14/birds-notes/">Bird Notes after a Year</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/jekyll/update/2023/02/22/naps/">Naps</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/jekyll/update/2023/02/01/update/">A Series of Life Observations</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/jekyll/update/2023/01/11/hedonistic-happy/">Hedonistic Treadmill: What It Means to Be Happy With What You Have</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/jekyll/update/2023/01/01/double-backprop/">Double Backpropagation: A Forgotten Algorithm</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/jekyll/update/2022/12/22/history/">PSA Reminder: History Actually Happened</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/jekyll/update/2022/12/10/beards/">My Experience Growing a Beard, And Then Shaving It</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/jekyll/update/2022/11/24/crawdads-tkam/">Comparison of Where The Crawdads Sing and To Kill a Mockingbird</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/jekyll/update/2022/11/09/word-association/">Food for Thought: Word Predictability</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/jekyll/update/2022/11/04/latex/">Why Does Latex look so good?</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/jekyll/update/2022/10/24/act/">Falling Forward, Not Back: Drake</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/jekyll/update/2022/10/11/gradient-descent-euler/">Gradient Descent Revisited As Euler's Method</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/jekyll/update/2022/10/04/birdslist/">Photograph Birds List</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/jekyll/update/2022/09/26/mltaylorseries-copy/">Hidden Taylor Series in Theoretical Machine Learning</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/jekyll/update/2022/09/25/paretoprinciple/">An explanation of Pareto's Principle</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/jekyll/update/2022/09/21/sveltevreact/">Yet Another Comparison of Svelte & React</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/jekyll/update/2022/09/20/closed-form-logreg/">Attempts at Closed-Form Logistic Regression</a>
          
      </details>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2023. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="home">Moments of Clarity</a>
            <small></small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="message" style="font-size: 0.9em">
  Hey y'all! Same blog, different (<a href="https://github.com/poole/lanyon"
    >Lanyon
  </a>
  themed) look. <br />
  If you are here for my theoretical articles or notes, they are
  <a href="/notes"> here</a>. <br />
  The sidebar also contains all my posts for easier navigation. <br />
  Hope you enjoy!
</div>

<div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="https://anish.lakkapragada.com/jekyll/update/2022/10/24/act/"> Falling Forward, Not Back: Drake </a>
    </h1>

    <span class="post-date">24 Oct 2022</span>

    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<p><strong>Keywords: Aubrey Drake Graham, Falling <del>Back</del> Off, Random</strong></p>

<blockquote>
  <p>This post is for pseudo-informational purposes and entertainment only.</p>
</blockquote>

<p>So by this point, youâ€™ve probably heard of Drake. Heâ€™s the guy who gave us the infamous Hotline bling meme, the most amount of <em>Billboard</em> Music Awards, and easily over a hundred grammies. Needless to say, he is a big deal.</p>

<p>However, a lot of people have argued that his music has been steadily declining for the last five years since 2015. I wouldnâ€™t go to that extent, but there is some truth in the fact that I find myself listening to a lot more of his past songs than his current songs. Maybe thatâ€™s a problem within rap itself, but for current rap music - Migos, 21 Savage, Juice WRLD (RIP) - I find myself listening to 2017-2019 songs compared to Drake - where I frequently go as back as 2011-2013.</p>

<p>Ironically, Headlines - easily one of Drakeâ€™s best songs - has this line in it:</p>

<blockquote>
  <p>I had someone tell me I fell off, ouh I needed that</p>
</blockquote>

<p>Yet somehow, still so many feel he fell off.</p>

<h2 id="the-good-drakes-best">The Good: Drakeâ€™s Best</h2>

<p>Letâ€™s start of with the good. Drake for sure has plentiful good songs - after all, you donâ€™t enjoy success without some talent.</p>

<p>A list of (my opinion ofc) Drakeâ€™s best songs are:</p>

<ul>
  <li><em>Headlines</em> (<strong>2011</strong>)</li>
  <li><em>Marvinâ€™s Room</em> (<strong>2011</strong>)</li>
  <li><em>Energy</em> (<strong>2015</strong>)</li>
  <li><em>Forever</em> (<strong>2009</strong>)</li>
  <li><em>Too Good</em> (<strong>2016</strong>)</li>
  <li><em>Do Not Disturb</em> (2017)</li>
  <li><em>Fake Love</em> (2017)</li>
  <li><em>Worst Behavior</em> (<strong>2013</strong>)</li>
  <li><em>From Time</em> (<strong>2013</strong>)</li>
  <li><em>Take Care</em> (<strong>2011</strong>)</li>
  <li><em>Hold On, Weâ€™re Going Home</em> (<strong>2013</strong>)</li>
  <li><em>Pound Cake + Paris Morton 2</em> (<strong>2013</strong>)</li>
  <li><em>A Keeper</em> (<strong>2022</strong>)</li>
  <li><em>Sticky</em> (2022)</li>
  <li><em>Portland</em> (2017)</li>
  <li><em>Falling Back</em> (2022)</li>
</ul>

<p><sup><sub>I didnâ€™t bold the 2017 songs, even though thatâ€™s five years ago!</sub></sup></p>

<p>You might be suprised by the last two. I do think those are decent songs by Drake in the future, and promising of perhaps a new wave of talent. Same goes with his featured verse on <em>Churchill Downs</em> - his verse was good to the point where videos have arised of his section only.</p>

<p>But from the list, I think itâ€™s clear for the most part weâ€™re 5 years past his prime.</p>

<h2 id="why-the-downfall">Why the downfall?</h2>

<p>Iâ€™m not really experienced in Drake or analyzing celebrities, but I think based on my preliminary research it mostly comes down to the lack of originality and high predictability.</p>

<p>Lot of people criticize for Drake not trying since his albums <em>Views From the Six</em>. Since then, he has released infamous albums <em>Certified Love Boy</em> and <em>Honestly, Nevermind</em>. Aside from <em>Falling Back</em> or <em>Sticky</em>, neither really had that great songs compared to his past. CLB in particular has been described as having boring beats and using a monotone, tuned out voice. This is in contrast to his past songs like <em>Marvinâ€™s Room</em> and <em>Headlines</em> which had more complex tunes and beats. <em>Worst Behavior</em> and <em>Energy</em> both also demonstrate a good use of a passionate voice than something with a tuned-out voice like <em>Girls Want Girls</em>. While the argument could be made that there is just a natural appreciation for loud, energetic sounds, a lack of diversity in an album is never a pro. Maybe this is just a meta point about rap songs by artists these days than anything else.</p>

<h2 id="is-this-to-be-expected">Is this to be expected?</h2>

<p>To some extent, yes. The pressure of being a star is insurmountable and expecting consistent hits is unreasonable. But, artists like Kendrick Lamar have delivered consistently creative songs. People wouldnâ€™t place this much attention to Drakeâ€™s supposed downfall if they didnâ€™t believe he couldnâ€™t do better.</p>

<p>This is the conclusion of this rant. Donâ€™t take anything here seriously.</p>


  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="https://anish.lakkapragada.com/jekyll/update/2022/10/11/gradient-descent-euler/"> Gradient Descent Revisited As Euler's Method </a>
    </h1>

    <span class="post-date">11 Oct 2022</span>

    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<p><strong>Keywords: Gradient Descent, Eulerâ€™s Method, Differential Equation</strong></p>

<p>Iâ€™ve already talked a fair amount about Gradient Descent. One of the most fascinating things about Gradient Descent is the amount of ways in which it secretly packs calculus concepts together. Aside from the fact that it literally uses <em>gradient</em>s, or partial derivatives, gradient descent is also derived from a <a href="2022-09-27-mltaylorseries%20copy.markdown">Taylor Series</a> which Iâ€™ve detailed before. Today Iâ€™d like to detail Gradient Descent in further detail as actually an application of Eulerâ€™s Method.</p>

<h2 id="gradient-descent-as-a-differential-equation">Gradient Descent as a Differential Equation</h2>

<p>As a refresher Gradient Descent is:</p>

\[\theta_{t} = \theta_{t - 1} - \alpha\nabla_{\theta_{t - 1}} J\]

<p>Unlike with the Taylor Series article, this time we will be keeping the learning rate \(\alpha\) into consideration.</p>

<p>Gradient Descent can actually be summarized as this Differential Equation.</p>

\[\dfrac{âˆ‚\theta_{t}}{âˆ‚t} = -\alpha \dfrac{âˆ‚J(\theta_{t})}{âˆ‚\theta_{t}}\]

<p>This differential equation is obviously unsolvable. A common way of approximating a function when an unsolvable differential equation is given is to use Eulerâ€™s method.</p>

<h2 id="primer-on-eulers-method">Primer on Eulerâ€™s Method</h2>

<p>Eulerâ€™s Method is a way to approximate any functionâ€™s value giving an initial starting point and itâ€™s differential equation. For the given \(y(0)=0\) and unsolvable differential equation \(\dfrac{dy}{dx} = \lvert x \rvert\), we can solve for \(y(2)\) by choosing a set number of iterations \(n\) and working our approximation up.</p>

<p>So for example if we want to do only 2 iterations, we would increase \(x\) by 1 each time. The step to first calculate \(y(1)\) is shown below.</p>

\[y(1) \approx y(0) + \Delta x*y'(0)\]

<p>where in this case \(y'(0)\) would represent the derivative of \(y\) at \(x=0\) and \(\Delta x\) gives the change in x (which in our case is 1). This would give us an approximation of \(y(1) \approx 0\).</p>

<p>We could redo this one more time and get \(y(2) \approx y(1) + y'(1)\), yielding \(y(2) \approx 1\). Of course this approximation is not accurate - however as the stepsize \(\Delta x\) approaches to 0 the approximations will get more accurate but there will be a lot more iterations. Hey, with gradient descent they always say a lower learning rate of \(\alpha\) does better but requires more iterations â€“ I sense some very big similarities.</p>

<h2 id="gradient-descent-conclusion">Gradient Descent Conclusion</h2>

<p>Letâ€™s look at our algo again - gradient descent is given by \(\theta_{t} = \theta_{t - 1} - \alpha\nabla_{\theta_{t - 1}} J\), thus we can see that it is basically eulerâ€™s method with a step size of \(\alpha\), however done in reverse (hence the negative in the step size.)</p>

<p>Yet another perspective on how to view Gradient Descent. More to come.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="https://anish.lakkapragada.com/jekyll/update/2022/10/04/birdslist/"> Photograph Birds List </a>
    </h1>

    <span class="post-date">04 Oct 2022</span>

    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<p><strong>Keywords: Bird Species, Random</strong></p>

<blockquote>
  <p>Note: These are all of birds near San Francisco, California. East-coast birds that I have seen are mostly excluded here.</p>
</blockquote>

<h2 id="bird-background">Bird Background</h2>

<p>For the last year, Iâ€™ve gone outside and taken photos of specifically birds (yes those things) in my backyard, neighborhood, and local parks and forests. Itâ€™s pretty fun and one of the bird things Iâ€™ve always wanted to do is to maintain a list of all the birdâ€™s Iâ€™ve photographed (the second is to make a tier list).</p>

<h2 id="birds-photographed">Birds Photographed</h2>

<ul>
  <li>California Scrub Jay</li>
  <li>Lesser Goldfinch</li>
  <li>Black Phoebe</li>
  <li>House Finch</li>
  <li>Western Bluebird</li>
  <li>American Robin</li>
  <li>White-Throated Sparrow</li>
  <li>Chestnut-Backed Chickadee</li>
  <li>Dark-eyed Junco</li>
  <li>California Towhee</li>
  <li>Mourning Dove</li>
  <li>White-crowned sparrow</li>
  <li>Gold-crowned sparrow</li>
  <li>American Robin</li>
  <li>Barn Swallow</li>
  <li>Yellow-rumped warbler</li>
  <li>Annaâ€™s Hummingbird</li>
  <li>Ruby Throated Hummingbird</li>
  <li>Bald Eagle</li>
  <li>Pine Siskin</li>
  <li>American Tree Sparrow</li>
  <li>Brown-headed cowbird</li>
  <li>Tree Swallow</li>
  <li>Acorn Woodpecker</li>
  <li>Brewerâ€™s Blackbird</li>
</ul>

<h2 id="locations-photographed">Locations Photographed</h2>

<p><em>Includes even one photograph.</em></p>

<ul>
  <li>Bay Area, California</li>
  <li>San Francisco, California</li>
  <li>Half Moon Bay, California</li>
  <li>Denali, Alaska</li>
  <li>Princeton, New Jersey</li>
  <li>Philadelphia, Pennsylvania</li>
  <li>Anne Arundel County, Maryland</li>
  <li>Ithaca, New York ðŸ˜‰</li>
</ul>

<h2 id="bird-tiers">Bird Tiers</h2>

<p>I will be ranking birds now in terms of my experiences photographing them and my general appreciation of their beauty in my photographs.</p>

<h2 id="tier-3-mid-birds">Tier 3: Mid Birds</h2>

<ul>
  <li>Brewerâ€™s Blackbird</li>
  <li>Pine Siskin</li>
  <li>Brown-Headed cowbird</li>
  <li>Mouring Dove</li>
  <li>California Towhee</li>
</ul>

<h2 id="tier-2-birds-that-ill-happily-take-a-photograph-of">Tier 2: Birds that Iâ€™ll happily take a photograph of</h2>

<ul>
  <li>Lesser Goldfinch</li>
  <li>Pine Siskin</li>
  <li>Black Phoebe</li>
  <li>Dark-Eyed Junco</li>
  <li>House Finch</li>
  <li>Western Bluebird</li>
  <li>White-crowned / Gold-crowned Sparrow</li>
  <li>Yellow-rumped warbler</li>
</ul>

<h2 id="tier-1-birds-that-are-rare">Tier 1: Birds that Are Rare</h2>

<ul>
  <li>California Scrub Jay</li>
  <li>American Robin</li>
  <li>Tree Swallow</li>
  <li>Barn Swallow</li>
  <li>Song Sparrow</li>
  <li>Bald Eagle</li>
  <li>Spotted Towhee</li>
  <li>American Tree Sparrow</li>
  <li>Acorn Woodpecker</li>
</ul>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="https://anish.lakkapragada.com/jekyll/update/2022/09/26/mltaylorseries-copy/"> Hidden Taylor Series in Theoretical Machine Learning </a>
    </h1>

    <span class="post-date">26 Sep 2022</span>

    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<p><strong>Keywords: Taylor Series, Calculus, Gradient Descent, Polynomial Regression, Theoretical ML</strong></p>

<p>This article hopes to provide an alternate look at some of the most foundational algorithms in machine learning, namely Gradient Descent and Polynomial Regression, from an angle of Taylor Series. 
Taylor Series are an extremely nice approximation method from calculus and are actually quite common in machine learning.</p>

<blockquote>
  <p>Machine Learning is about creating functions to model (i.e. approximate) functions in data - Taylor series is about approximating functions as well; it should come as no suprise that they are related in many cases.</p>
</blockquote>

<h2 id="taylor-series-primer">Taylor Series Primer</h2>

<p>So, what are taylor series? I wonâ€™t go into the proof here, because I donâ€™t remember it and I donâ€™t think its required, but the main detail is that Taylor Series are a way to approximate any function \(f(x)\) at a given point \(c\) using an infinite sum of polynomials. Itâ€™s formula is shown below:</p>

\[f(x)=\sum_{n=0}^\infty f^{(n)}(c)\frac{(x-c)^n}{n!} = f(c) + f^{(1)}(c)(x-c) + \dfrac{f^{(2)}(c)(x-c)^2}{2} + \ldots\]

<p>Where \(f^{(n)}(c)\) represents the \(n\)-th derivative (or just \(f(c)\) if \(n\) is 0), at a point of \(x=c\). Above, we only explicitly show the summation up to the second degree term - if we were to remove the other terms \(\ldots\), we would be left with the <em>second-degree approximation</em> of \(f(x)\). Using finite approximations of a taylor series will become important later, as infinite summations are not always possible in a computer.</p>

<p>On with the examples!</p>

<h2 id="taylor-series-in-gradient-descent">Taylor Series in Gradient Descent</h2>

<p><em>Before reading about the usage of Taylor Series in gradient descent, keep in mind that many different calculus concepts (not just taylor series!) play into gradient descent.</em></p>

<p>Gradient Descent is an iterative algorithm to generally optimize some function overtime based on its gradient (vector of partial derivatives.) The gradient yields a vector that tells the fastest way to ascend a curve - with gradient <em>descent</em> you try to go <em>down</em> the curve and thus constantly move in the negative of this gradient (moving with the positive is called gradient ascent). Gradient Descent is shown below.</p>

\[\theta_{t} = \theta_{t - 1}  - \alpha * \nabla_{\theta_{t - 1}} J\]

<p>For more clarification, \(\theta_{t}\) are the parameters of the model (gradient descent is model agnostic, so this could range from linear regression to GPT-3) at a timestep \(t\) and \(\nabla_{\theta_{t - 1}} J\) represents the gradient of the objective function \(J\) that we are trying to minimize. Itâ€™s not shown, but \(J\) takes in the parameters \(x, y\) and \(\theta\).  As stated, we move in the direction of the negative of the gradient. \(\alpha\) is the learning rate and is applied to scale the gradient and prevent too high movements.</p>

<p>Gradient Descent can be re-thought of as finding some value \(\Delta \theta\) to adjust \(\theta_{t - 1}\) at each iteration \(t\) such that \(J(\theta_{t - 1} + \Delta \theta)\) is less than \(J(\theta_{t - 1})\). This is where taylor series come in - they help us find this required change. The taylor series to approximate the new value of the objective function to the first-degree is shown below.</p>

<blockquote>
  <p>Disclaimer: My linear algebra and multivariable calculus skills are kinda DNE, so please donâ€™t try to inspect every term and make sure that the shapes all match up (probably missing a transpose here and there). Instead try to built intuition of the general approach.</p>
</blockquote>

\[J(\theta_{t - 1} + \Delta \theta) \approx J(\theta_{t - 1}) + \nabla_{\theta_{t - 1}} J * \Delta \theta\]

<p>This is a bit confusing, so letâ€™s clarify, in this case the \(x\) in the approximation of \(f(x)\) is actually \(\theta_{t - 1} + \Delta \theta\) and thus the taylor series is centered at a point \(c\) of \(\theta_{t - 1}\). This means that the first-degree term \(x - c\) actually equals just \(\Delta \theta\).</p>

<p>This is where it gets clever. Given that we set \(J(\theta_{t - 1})\) or \(J(\theta_{t - 1} + \Delta \theta)\) to be less than \(J(\theta_{t - 1})\), to make both sides of the approximation equal, we basically need to find a way to balance the right and left side by reducing \(J(\theta_{t - 1})\) with negatives from the first-degree expression (\(\nabla_{\theta_{t - 1}} J * \Delta\theta\)) to it. We could just set \(\Delta \theta\) to \(-1\), but because \(\nabla_{\theta{t - 1}} J\) is a matrix - there is no guarantee that all of the numbers inside of it will necessarily be the same sign. Thus we need to find some value of \(\Delta \theta\) that will make \(\nabla_{\theta{t - 1}} J\) ALL negative.</p>

<p>This is actually much easier than expected. Just set \(\Delta \theta\) to the <em>negative</em> of \(\nabla_{\theta{t - 1}} J\) and we guarantee all elements are negative (as a gradient squared yields all positive values.) This leads to the approximation having any chance of balancing out. As we add more degrees to the taylor polynomial, this approximation would keep on getting better.</p>

<p>This means that in our (minimization) algorithm of Gradient Descent, with \(\alpha\) for stability, we have:</p>

\[\theta_{t} = \theta_{t - 1} + \Delta \theta _{t - 1} = \theta_{t - 1} - \alpha * \nabla_{\theta{t - 1}} J\]

<p>Thatâ€™s gradient descent!</p>

<p>A lot of this information was taken from <a href="https://www.cs.princeton.edu/courses/archive/fall18/cos597G/lecnotes/lecture3.pdf">here</a> and summarized to be less confusing and more intuitive here. We can expand this method to actually include second and third derivatives as well - but those arenâ€™t used as much due to requiring more computational power (sorry for the explanation that literally <em>everyone</em> gives.)</p>

<h2 id="taylor-series-in-polynomial-regression">Taylor Series in Polynomial Regression</h2>

<p>Weeee that was a lot of work for gradient descent! Luckily, applications of taylor series in polynomial regression is much more straightforward.</p>

<p>Polynomial regression is basically a type of linear regression where a single input (letâ€™s stick to one-dimensional input for simplicity) is raised to higher powers and then the appropriate coefficients are found. The prediction function for a two-degree polynomial regression is shown below.</p>

\[\hat{y} = h(x) = \beta_{0} + \beta_{1}x + \beta_{2}x^{2}\]

<p>As shown above, \(\hat{y}\) are the predictions and \(\beta_{n}\) are the coefficients for \(x\) raised to the \(n\)-th power. Already looks like a taylor series (or maclaurin series as \(c = 0\)) right?</p>

<p>In fact, it is! Itâ€™s that simple. Letâ€™s see if this actually works in practice.</p>

<h3 id="empirical-experiment-with-exponentials">Empirical Experiment with Exponentials</h3>

<p>Alliteration, huh? Okay, so the famous taylor series of the function \(e^{x}\) is shown below.</p>

\[e^{x} = \sum_{n = 0}^{\infty} \dfrac{x^{n}}{n!} = 1 + x + \dfrac{x^{2}}{2} + \ldots\]

<p>Will Polynomial Regression (to a degree of 2) actually learn this specific taylor series? To reiterate, if I train a Linear Regression model with <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html">scikit-learn</a>â€™s in Python, that takes in \(x\) and \(x^{2}\) as input - will it learn \({1, 1, 0.5}\) as \(\beta_{0}, \beta_{1}, \beta_{2}\) respectively?</p>

<p>Will this taylor series be learned, from an algorithm derived from taylor series ðŸ¤¯ ?</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span> 

<span class="s">"""
we'll generate from a normal distribution as the 
second-degree polynomial approximates best in this range.
"""</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="p">))</span> <span class="c1"># 1000 standard normal samples 
</span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">power</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">e</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="c1"># labels
</span></code></pre></div></div>

<p>We can now construct the \(X^{2}\) data. Weâ€™ll merge into <code class="language-plaintext highlighter-rouge">X_poly</code> which will contain two columns - the first one for \(X\) and the second one for \(X^{2}\).</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">power</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">X_poly</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">((</span><span class="n">X</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">X_2</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>Letâ€™s apply Linear Regression.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lin_reg</span> <span class="o">=</span> <span class="nc">LinearRegression</span><span class="p">()</span> <span class="c1"># lin reg algorithm 
</span><span class="n">lin_reg</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_poly</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<p>Letâ€™s inspect the coefficients \(\beta_{1}, \beta_{2}\).</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">lin_reg</span><span class="p">.</span><span class="n">coef_</span>
<span class="nf">array</span><span class="p">([</span><span class="o">-</span><span class="mf">1.24106858</span><span class="p">,</span>  <span class="mf">2.25427569</span><span class="p">])</span>
</code></pre></div></div>
<p>And the bias \(\beta_{0}\).</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">lin_reg</span><span class="p">.</span><span class="n">intercept_</span>
<span class="mf">1.5053263303981406</span>
</code></pre></div></div>

<p>Looks like it wonâ€™t.</p>

<p>My hypothesis is that if you use a higher-degree approximation (and of course have much more data), it will be more likely the coefficients will slowly fall in line with the <em>true</em> taylor polynomial. In our case, given that it only had a sample of \(e^{x}\) and not the whole infinite set of values, it is to be expected that not the precise values were found as these values actually could have minimized the mean-squared error more for this specific training set \(X\).</p>

<p>Also,  if Polynomial Regression is a taylor series polynomial then linear regression is a first-degree taylor series?</p>

<h2 id="review">Review</h2>

<p>Congrats on making it here!</p>

<p>Taylor Series are a great way to approximate any function into polynomials. They have a lot of hidden applications in machine learning mathematics. We went over their role in helping determine how to move iteratively in Gradient Descent and itâ€™s very clear application to Polynomial Regression.</p>

<p>Please let me know what more theoretical machine learning applications you find! Thanks for reading.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="https://anish.lakkapragada.com/jekyll/update/2022/09/25/paretoprinciple/"> An explanation of Pareto's Principle </a>
    </h1>

    <span class="post-date">25 Sep 2022</span>

    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<p><strong>Keywords: Paretoâ€™s Principle, 80/20 rule, Distribution of Outcomes, Applications</strong></p>

<p>A while ago (more like 4 years ago) I discovered Paretoâ€™s principle - also known as the 80/20 rule - which states thatâ€™s 80% of outputs result from 20% of the inputs. Less abstractly, this means that 20% of the input to anything, such as time or effort, will lead to 80% of the actual outputs. For example, only 20% of the features on Microsoft Word will actually be used by 80% of the users whereas the remaining 80% of the features will be used by 20% of the users.</p>

<p>Paretoâ€™s distribution is stunningly applicable. 80% of the crimes are commited by 20% of registered criminals, and 80% of a businessâ€™s wealth comes from only 20% of the clients. 20% of the apps on your phone likely are used 80% of the time. This distribution does not need to be exactly 80/20; in some cases it can even go down to 95/5 or even 99/1 (e.g. distribution of wealth across population).</p>

<p>So, how can we use Paretoâ€™s principle in our daily lives? Internalizing the exponential nature in our lives is probably the best way to implement it. If we want a 90 on a test, weâ€™ll only need to put in about half of the effort and time studying compared to getting a 97+. The same goes for standardizing testing; scores are gradually harder to get at the top. Keeping in mind the 80/20 rule can help us decide when the extra mile is excessive or essential.</p>

<p>So now weâ€™ve gone over the how and what - only the <em>why</em> remains. Any ideas on why our world and outcomes are distributed this way? My guess is that the underlying reason of the 80/20 rule is disparity and inequity - one group having much more extreme outliers that overpower all others. The world is moving at an <a href="https://www.su.org/blog/thriving-in-an-exponential-world-and-making-a-difference-doing-so">exponential rate</a>; the most successful stocks (which are extremes in themselves) almost always follow exponential curves as compared to linear ascents. Mooreâ€™s law literally forecasts that the number of transistors on a microchip double every 2 years; itâ€™s no secret that <a href="https://en.wikipedia.org/wiki/Accelerating_change">we also think the world is constantly growing faster</a> than it ever has before. In short, Paretoâ€™s principle seems to be very common when disparity or a gap grows uncontrollably. In our world, thatâ€™s not too uncommon.</p>


  </div>
  
</div>

<div class="pagination">
  
  <a
    class="pagination-item older"
    href="https://anish.lakkapragada.com/page5"
    >Older</a
  >
    
  <a
    class="pagination-item newer"
    href="https://anish.lakkapragada.com/page3"
    >Newer</a
  >
   
</div>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/public/js/script.js'></script>
  </body>
</html>
